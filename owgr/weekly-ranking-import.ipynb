{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "- parse (Am) out of player name as is_amatuer\n",
    "- parse first and last names from player_name\n",
    "- import row by row into MySql (on SiteGround)\n",
    "- build 2nd script to parse and import full weekly data\n",
    "- redo weekly ranking download to csv and new filename (owgr-20190310.csv)\n",
    "- figure out how to schedule weekly download of rankings and import\n",
    "- build API for historical OWGR rankings and publish\n",
    "- setup weekly scheduler on laptop to push to remote db (automate on server ideally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pyodbc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKINGS_URL = \"http://www.owgr.com/en/Ranking.aspx?pageNo={0}&pageSize=300&country=All\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ranking_table(soup):\n",
    "    output = []\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        col_idx = 0\n",
    "        data = {}\n",
    "\n",
    "        if len(cols) > 0:\n",
    "            for col in cols:\n",
    "                text = col.text.strip()\n",
    "                if (text == '-'):\n",
    "                    text = '0'\n",
    "\n",
    "                if col_idx == 0:\n",
    "                    data['current_week_rank'] = text\n",
    "                elif col_idx == 1:\n",
    "                    data['last_week_rank'] = text\n",
    "                elif col_idx == 3:\n",
    "                    img = col.find('img')\n",
    "                    data['country'] = img['title']\n",
    "                elif col_idx == 4:\n",
    "                    data['player_name'] = text\n",
    "                    u = col.find('a')\n",
    "                    data['player_id'] = u['href'].rsplit('=')[1]\n",
    "                elif col_idx == 5:\n",
    "                    data['avg_points'] = text\n",
    "                elif col_idx == 6:\n",
    "                    data['total_points'] = text\n",
    "                elif col_idx == 7:\n",
    "                    data['events_played_divisor'] = text\n",
    "                elif col_idx == 10:\n",
    "                    data['events_played_all'] = text\n",
    "\n",
    "                col_idx = col_idx + 1\n",
    "\n",
    "            output.append(data)\n",
    "            \n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_ranking_week(soup):\n",
    "    h2 = soup.find_all('section', attrs={'id': 'ranking_table'})[0].find_all('h2')\n",
    "    t = soup.find_all('time', attrs={'class':'sub_header'})\n",
    "    \n",
    "    week_num = int(h2[0].text.replace('WEEK', '').strip())\n",
    "    \n",
    "    temp = t[0].text.strip().split(' ')\n",
    "    temp[0] = temp[0][:-2] # remove th, nd, st\n",
    "    week_date = datetime.strptime(str(temp[1]) + ' ' + str(temp[0]) + ' ' + str(temp[2]), '%B %d %Y')\n",
    "    \n",
    "    return week_num, week_date\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    request = urllib.request.urlopen(url)\n",
    "    html = request.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def download_ranking_table(url):\n",
    "    soup = get_soup(url)\n",
    "    table = parse_ranking_table(soup)\n",
    "    week_num, week_date = parse_ranking_week(soup)\n",
    "    \n",
    "    return week_num, week_date, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_current_week():\n",
    "    # get current week date\n",
    "    soup = get_soup(RANKINGS_URL.replace('{0}', \"1\"))\n",
    "    week_num, week_date = parse_ranking_week(soup)\n",
    "    output_file = '../data/owgr-' + str(week_date.year) + '{:02d}'.format(week_date.month) + '{:02d}'.format(week_date.day) + '.csv'\n",
    "    \n",
    "    # create header and start of csv file\n",
    "    header = [['player_id', 'player_name', 'country', 'week_of', 'rank', 'avg_points', 'total_points', 'events_played_divisor', 'events_played_all']]\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(header)\n",
    "        f.close()\n",
    "        \n",
    "    # loop through all pages and save data as csv\n",
    "    for i in range(1, 31):\n",
    "        print('Downloading page', str(i))\n",
    "        soup = get_soup(RANKINGS_URL.replace('{0}', str(i)))\n",
    "        output = parse_ranking_table(soup)\n",
    "\n",
    "        csv_data = []\n",
    "        for item in output:\n",
    "            item_row = []\n",
    "            item_row.append(str(item['player_id']))\n",
    "            item_row.append(item['player_name'])\n",
    "            item_row.append(item['country'])\n",
    "            item_row.append(str(week_date))\n",
    "            item_row.append(str(item['current_week_rank']))\n",
    "            item_row.append(str(item['avg_points']))\n",
    "            item_row.append(str(item['total_points']))\n",
    "            item_row.append(str(item['events_played_divisor']))\n",
    "            item_row.append(str(item['events_played_all']))\n",
    "            csv_data.append(item_row)\n",
    "\n",
    "        with open(output_file, 'a+', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(csv_data)\n",
    "            f.close()\n",
    "    \n",
    "    print('Done.')\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_full_name(name):\n",
    "    is_amatuer = '(Am)' in name\n",
    "    \n",
    "    name = name.replace('(Am)', '').split(' ')\n",
    "    if len(name) > 1:\n",
    "        first_name = name[0]\n",
    "        last_name = ' '.join(name[1:])\n",
    "    else:\n",
    "        first_name = name[0]\n",
    "        last_name = ''\n",
    "        \n",
    "    return first_name.strip(), last_name.strip(), is_amatuer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_ranking_file(filepath):\n",
    "    # open file and get all ranking data\n",
    "    with open(filepath, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        history = list(reader)\n",
    "\n",
    "    # loop through file and execute sql statements\n",
    "    conn = pyodbc.connect(\"Driver={SQL Server Native Client 11.0}; Server=THISRON; Database=ShotLink; Trusted_Connection=yes;\", \n",
    "                          autocommit=True)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for row in history[1:]:\n",
    "        if len(row) == 9: # full weekly file\n",
    "            player_id = row[0]\n",
    "            player_name = row[1]\n",
    "            country = row[2]\n",
    "            week_of = row[3]\n",
    "            ranking = int(row[4])\n",
    "            avg_points = float(row[5])\n",
    "            total_points = float(row[6])\n",
    "            events_played_divisor = int(row[7])\n",
    "            events_played_all = int(row[8])\n",
    "            \n",
    "        if len(row) == 4: # full history file\n",
    "            player_id = row[0]\n",
    "            player_name = row[1]\n",
    "            country = ''\n",
    "            week_of = row[2]\n",
    "            ranking = int(row[3])\n",
    "            avg_points = 0.0\n",
    "            total_points = 0.0\n",
    "            events_played_divisor = 0\n",
    "            events_played_all = 0\n",
    "            \n",
    "        first_name, last_name, is_amatuer = parse_full_name(player_name)\n",
    "        \n",
    "        sql = 'exec [ShotLink].[dbo].[OWGR_AddRanking] ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?'\n",
    "        values = (\n",
    "            str(player_id), player_name.replace('(Am)', '').strip(), first_name, last_name, country, str(is_amatuer),\n",
    "            week_of, str(ranking), str(avg_points), str(total_points), str(events_played_divisor), str(events_played_all)\n",
    "        )\n",
    "        cursor.execute(sql, (values))\n",
    "        \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 1\n",
      "Downloading page 2\n",
      "Downloading page 3\n",
      "Downloading page 4\n",
      "Downloading page 5\n",
      "Downloading page 6\n",
      "Downloading page 7\n",
      "Downloading page 8\n",
      "Downloading page 9\n",
      "Downloading page 10\n",
      "Downloading page 11\n",
      "Downloading page 12\n",
      "Downloading page 13\n",
      "Downloading page 14\n",
      "Downloading page 15\n",
      "Downloading page 16\n",
      "Downloading page 17\n",
      "Downloading page 18\n",
      "Downloading page 19\n",
      "Downloading page 20\n",
      "Downloading page 21\n",
      "Downloading page 22\n",
      "Downloading page 23\n",
      "Downloading page 24\n",
      "Downloading page 25\n",
      "Downloading page 26\n",
      "Downloading page 27\n",
      "Downloading page 28\n",
      "Downloading page 29\n",
      "Downloading page 30\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "filepath = download_current_week()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import_ranking_file('../data/owgr-20190310.csv')\n",
    "#import_ranking_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import_ranking_file('../data/owgr-history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/owgr-20190324.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import_ranking_file('../data/owgr-20190317.csv')\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import_ranking_file('../data/owgr-20190310.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import_ranking_file('../data/owgr-20190317.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_ranking_file(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
